{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled0.ipynb",
      "provenance": [],
      "mount_file_id": "1PEsxD_RKVHtiKibRhy5psHW9n14b7HDY",
      "authorship_tag": "ABX9TyOWut8CMsXYsxtFvn39HN9o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuixianZhang/Vision-Transformer-Covid/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEsJgCe_tczG",
        "outputId": "3acb3296-ff60-484b-dc71-6cf7007bd863"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJIM8tUOtqPV",
        "outputId": "a9b82ded-ab81-4d08-8dc8-125dddca2622"
      },
      "source": [
        "import os\n",
        "print(os.listdir('/content/drive/MyDrive'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Colab Notebooks', 'train_new', 'train_new.csv', 'test_new']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5Y2Cl6ktqQf",
        "outputId": "5a1a4e29-4e5e-491f-d23b-fdfb17284152"
      },
      "source": [
        "!pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktBdGXJQFkjg",
        "outputId": "6b08ba26-1430-459b-910e-828cd2aa347e"
      },
      "source": [
        "!pip install vit-keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vit-keras\n",
            "  Downloading vit_keras-0.1.0-py3-none-any.whl (24 kB)\n",
            "Collecting validators\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from vit-keras) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->vit-keras) (1.19.5)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->vit-keras) (4.4.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->vit-keras) (1.15.0)\n",
            "Installing collected packages: validators, vit-keras\n",
            "Successfully installed validators-0.18.2 vit-keras-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7u-UqtOFmYs",
        "outputId": "8bbed89e-9c03-4087-f444-a8c294fb5094"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import glob, warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from vit_keras import vit\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print('TensorFlow Version ' + tf.__version__)\n",
        "\n",
        "IMAGE_SIZE = 256\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 20\n",
        "\n",
        "TRAIN_PATH = '/content/drive/MyDrive/train_new'\n",
        "TEST_PATH = '/content/drive/MyDrive/test_new'\n",
        "\n",
        "DF_TRAIN = pd.read_csv('/content/drive/MyDrive/train_new.csv', dtype='str')\n",
        "TEST_IMAGES = glob.glob(TEST_PATH + '/*.png')\n",
        "DF_TEST = pd.DataFrame(TEST_IMAGES, columns = ['image_path'])\n",
        "\n",
        "classes = {0 : \"COVID\",\n",
        "           1 : \"Lung_Opacity\",\n",
        "           2 : \"Normal\",\n",
        "           3 : \"Viral Pneumonia\"}\n",
        "\n",
        "def data_augment(image):\n",
        "    p_spatial = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
        "    p_rotate = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
        "    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
        "    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
        "    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
        "    \n",
        "    # Flips\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    \n",
        "    if p_spatial > .75:\n",
        "        image = tf.image.transpose(image)\n",
        "        \n",
        "    # Rotates\n",
        "    if p_rotate > .75:\n",
        "        image = tf.image.rot90(image, k = 3) # rotate 270º\n",
        "    elif p_rotate > .5:\n",
        "        image = tf.image.rot90(image, k = 2) # rotate 180º\n",
        "    elif p_rotate > .25:\n",
        "        image = tf.image.rot90(image, k = 1) # rotate 90º\n",
        "        \n",
        "    # Pixel-level transforms\n",
        "    if p_pixel_1 >= .4:\n",
        "        image = tf.image.random_saturation(image, lower = .7, upper = 1.3)\n",
        "    if p_pixel_2 >= .4:\n",
        "        image = tf.image.random_contrast(image, lower = .8, upper = 1.2)\n",
        "    if p_pixel_3 >= .4:\n",
        "        image = tf.image.random_brightness(image, max_delta = .1)\n",
        "        \n",
        "    return image\n",
        "\n",
        "\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n",
        "                                                          samplewise_center = True,\n",
        "                                                          samplewise_std_normalization = True,\n",
        "                                                          validation_split = 0.2,\n",
        "                                                          preprocessing_function = data_augment)\n",
        "\n",
        "train_gen = datagen.flow_from_dataframe(dataframe = DF_TRAIN,\n",
        "                                        directory = TRAIN_PATH,\n",
        "                                        x_col = 'image_id',\n",
        "                                        y_col = 'label',\n",
        "                                        subset = 'training',\n",
        "                                        batch_size = BATCH_SIZE,\n",
        "                                        seed = 1,\n",
        "                                        color_mode = 'rgb',\n",
        "                                        shuffle = True,\n",
        "                                        class_mode = 'categorical',\n",
        "                                        target_size = (IMAGE_SIZE, IMAGE_SIZE))\n",
        "\n",
        "valid_gen = datagen.flow_from_dataframe(dataframe = DF_TRAIN,\n",
        "                                        directory = TRAIN_PATH,\n",
        "                                        x_col = 'image_id',\n",
        "                                        y_col = 'label',\n",
        "                                        subset = 'validation',\n",
        "                                        batch_size = BATCH_SIZE,\n",
        "                                        seed = 1,\n",
        "                                        color_mode = 'rgb',\n",
        "                                        shuffle = False,\n",
        "                                        class_mode = 'categorical',\n",
        "                                        target_size = (IMAGE_SIZE, IMAGE_SIZE))\n",
        "\n",
        "test_gen = datagen.flow_from_dataframe(dataframe = DF_TEST,\n",
        "                                       x_col = 'image_path',\n",
        "                                       y_col = None,\n",
        "                                       batch_size = BATCH_SIZE,\n",
        "                                       seed = 1,\n",
        "                                       color_mode = 'rgb',\n",
        "                                       shuffle = False,\n",
        "                                       class_mode = None,\n",
        "                                       target_size = (IMAGE_SIZE, IMAGE_SIZE))\n",
        "# images = [train_gen[0][0][i] for i in range(16)]\n",
        "# fig, axes = plt.subplots(3, 5, figsize = (10, 10))\n",
        "\n",
        "# axes = axes.flatten()\n",
        "\n",
        "# for img, ax in zip(images, axes):\n",
        "#     ax.imshow(img.reshape(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "#     ax.axis('off')\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# !pip install --quiet vit-keras\n",
        "\n",
        "\n",
        "\n",
        "vit_model = vit.vit_b32(\n",
        "        image_size = IMAGE_SIZE,\n",
        "        activation = 'softmax',\n",
        "        pretrained = True,\n",
        "        include_top = False,\n",
        "        pretrained_top = False,\n",
        "        classes = 4)\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "        vit_model,\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(11, activation = tfa.activations.gelu),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(4, 'softmax')\n",
        "    ],\n",
        "    name = 'vision_transformer')\n",
        "\n",
        "model.summary()\n",
        "learning_rate = 1e-4\n",
        "\n",
        "optimizer = tfa.optimizers.RectifiedAdam(learning_rate = learning_rate)\n",
        "\n",
        "model.compile(optimizer = optimizer, \n",
        "              loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2), \n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "STEP_SIZE_TRAIN = train_gen.n // train_gen.batch_size\n",
        "STEP_SIZE_VALID = valid_gen.n // valid_gen.batch_size\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',\n",
        "                                                 factor = 0.2,\n",
        "                                                 patience = 2,\n",
        "                                                 verbose = 1,\n",
        "                                                 min_delta = 1e-4,\n",
        "                                                 min_lr = 1e-6,\n",
        "                                                 mode = 'max')\n",
        "\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n",
        "                                                 min_delta = 1e-4,\n",
        "                                                 patience = 5,\n",
        "                                                 mode = 'max',\n",
        "                                                 restore_best_weights = True,\n",
        "                                                 verbose = 1)\n",
        "\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = '/content/drive/MyDrive/model_vit_covid.hdf5',\n",
        "                                                  monitor = 'val_accuracy', \n",
        "                                                  verbose = 1, \n",
        "                                                  save_best_only = True,\n",
        "                                                  save_weights_only = True,\n",
        "                                                  mode = 'max')\n",
        "\n",
        "callbacks = [earlystopping, reduce_lr, checkpointer]\n",
        "\n",
        "model.fit(x = train_gen,\n",
        "          steps_per_epoch = STEP_SIZE_TRAIN,\n",
        "          validation_data = valid_gen,\n",
        "          validation_steps = STEP_SIZE_VALID,\n",
        "          epochs = EPOCHS,\n",
        "          callbacks = callbacks)\n",
        "\n",
        "# model.save('model.h5', save_weights_only = True)\n",
        "model.save('/content/drive/MyDrive/model_vit_covid.h5')\n",
        "predicted_classes = np.argmax(model.predict(valid_gen, steps = valid_gen.n // valid_gen.batch_size + 1), axis = 1)\n",
        "true_classes = valid_gen.classes\n",
        "class_labels = list(valid_gen.class_indices.keys())  \n",
        "\n",
        "# confusionmatrix = confusion_matrix(true_classes, predicted_classes)\n",
        "# plt.figure(figsize = (16, 16))\n",
        "# sns.heatmap(confusionmatrix, cmap = 'Blues', annot = True, cbar = True)\n",
        "\n",
        "print(classification_report(true_classes, predicted_classes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version 2.7.0\n",
            "Found 13546 validated image filenames belonging to 4 classes.\n",
            "Found 3386 validated image filenames belonging to 4 classes.\n",
            "Found 4233 validated image filenames.\n",
            "Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-B_32_imagenet21k+imagenet2012.npz\n",
            "353255424/353253686 [==============================] - 4s 0us/step\n",
            "353263616/353253686 [==============================] - 4s 0us/step\n",
            "Model: \"vision_transformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vit-b32 (Functional)        (None, 768)               87466752  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 768)               0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 768)              3072      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 11)                8459      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 11)               44        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 48        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 87,478,375\n",
            "Trainable params: 87,476,817\n",
            "Non-trainable params: 1,558\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "846/846 [==============================] - ETA: 0s - loss: 1.0503 - accuracy: 0.7234\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.85634, saving model to /content/drive/MyDrive/model_vit_covid.hdf5\n",
            "846/846 [==============================] - 3667s 4s/step - loss: 1.0503 - accuracy: 0.7234 - val_loss: 0.8687 - val_accuracy: 0.8563 - lr: 1.0000e-04\n",
            "Epoch 2/20\n",
            "846/846 [==============================] - ETA: 0s - loss: 0.8461 - accuracy: 0.8605\n",
            "Epoch 00002: val_accuracy improved from 0.85634 to 0.90581, saving model to /content/drive/MyDrive/model_vit_covid.hdf5\n",
            "846/846 [==============================] - 715s 845ms/step - loss: 0.8461 - accuracy: 0.8605 - val_loss: 0.7611 - val_accuracy: 0.9058 - lr: 1.0000e-04\n",
            "Epoch 3/20\n",
            "846/846 [==============================] - ETA: 0s - loss: 0.8018 - accuracy: 0.8812\n",
            "Epoch 00003: val_accuracy did not improve from 0.90581\n",
            "846/846 [==============================] - 708s 837ms/step - loss: 0.8018 - accuracy: 0.8812 - val_loss: 0.7577 - val_accuracy: 0.8975 - lr: 1.0000e-04\n",
            "Epoch 4/20\n",
            "846/846 [==============================] - ETA: 0s - loss: 0.7696 - accuracy: 0.8965\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90581\n",
            "846/846 [==============================] - 706s 835ms/step - loss: 0.7696 - accuracy: 0.8965 - val_loss: 0.7376 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
            "Epoch 5/20\n",
            "846/846 [==============================] - ETA: 0s - loss: 0.7167 - accuracy: 0.9305\n",
            "Epoch 00005: val_accuracy improved from 0.90581 to 0.93276, saving model to /content/drive/MyDrive/model_vit_covid.hdf5\n",
            "846/846 [==============================] - 710s 839ms/step - loss: 0.7167 - accuracy: 0.9305 - val_loss: 0.7032 - val_accuracy: 0.9328 - lr: 2.0000e-05\n",
            "Epoch 6/20\n",
            "846/846 [==============================] - ETA: 0s - loss: 0.7043 - accuracy: 0.9376\n",
            "Epoch 00006: val_accuracy improved from 0.93276 to 0.93602, saving model to /content/drive/MyDrive/model_vit_covid.hdf5\n",
            "846/846 [==============================] - 710s 839ms/step - loss: 0.7043 - accuracy: 0.9376 - val_loss: 0.7004 - val_accuracy: 0.9360 - lr: 2.0000e-05\n",
            "Epoch 7/20\n",
            "846/846 [==============================] - ETA: 0s - loss: 0.6991 - accuracy: 0.9414\n",
            "Epoch 00007: val_accuracy did not improve from 0.93602\n",
            "846/846 [==============================] - 708s 836ms/step - loss: 0.6991 - accuracy: 0.9414 - val_loss: 0.7098 - val_accuracy: 0.9289 - lr: 2.0000e-05\n",
            "Epoch 8/20\n",
            "846/846 [==============================] - ETA: 0s - loss: 0.6949 - accuracy: 0.9426\n",
            "Epoch 00008: val_accuracy improved from 0.93602 to 0.93898, saving model to /content/drive/MyDrive/model_vit_covid.hdf5\n",
            "846/846 [==============================] - 709s 838ms/step - loss: 0.6949 - accuracy: 0.9426 - val_loss: 0.6908 - val_accuracy: 0.9390 - lr: 2.0000e-05\n",
            "Epoch 9/20\n",
            "846/846 [==============================] - ETA: 0s - loss: 0.6853 - accuracy: 0.9482\n",
            "Epoch 00009: val_accuracy did not improve from 0.93898\n",
            "846/846 [==============================] - 703s 831ms/step - loss: 0.6853 - accuracy: 0.9482 - val_loss: 0.7016 - val_accuracy: 0.9336 - lr: 2.0000e-05\n",
            "Epoch 10/20\n",
            "846/846 [==============================] - ETA: 0s - loss: 0.6798 - accuracy: 0.9527\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.93898\n",
            "846/846 [==============================] - 708s 837ms/step - loss: 0.6798 - accuracy: 0.9527 - val_loss: 0.6971 - val_accuracy: 0.9369 - lr: 2.0000e-05\n",
            "Epoch 11/20\n",
            "846/846 [==============================] - ETA: 0s - loss: 0.6679 - accuracy: 0.9602\n",
            "Epoch 00011: val_accuracy did not improve from 0.93898\n",
            "846/846 [==============================] - 711s 841ms/step - loss: 0.6679 - accuracy: 0.9602 - val_loss: 0.6862 - val_accuracy: 0.9390 - lr: 4.0000e-06\n",
            "Epoch 12/20\n",
            "846/846 [==============================] - ETA: 0s - loss: 0.6616 - accuracy: 0.9633\n",
            "Epoch 00012: val_accuracy improved from 0.93898 to 0.94491, saving model to /content/drive/MyDrive/model_vit_covid.hdf5\n",
            "846/846 [==============================] - 713s 843ms/step - loss: 0.6616 - accuracy: 0.9633 - val_loss: 0.6812 - val_accuracy: 0.9449 - lr: 4.0000e-06\n",
            "Epoch 13/20\n",
            "846/846 [==============================] - ETA: 0s - loss: 0.6594 - accuracy: 0.9662\n",
            "Epoch 00013: val_accuracy did not improve from 0.94491\n",
            "846/846 [==============================] - 705s 833ms/step - loss: 0.6594 - accuracy: 0.9662 - val_loss: 0.6812 - val_accuracy: 0.9414 - lr: 4.0000e-06\n",
            "Epoch 14/20\n",
            "846/846 [==============================] - ETA: 0s - loss: 0.6612 - accuracy: 0.9645\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.94491\n",
            "846/846 [==============================] - 705s 833ms/step - loss: 0.6612 - accuracy: 0.9645 - val_loss: 0.6855 - val_accuracy: 0.9419 - lr: 4.0000e-06\n",
            "Epoch 15/20\n",
            "514/846 [=================>............] - ETA: 4:10 - loss: 0.6575 - accuracy: 0.9690"
          ]
        }
      ]
    }
  ]
}